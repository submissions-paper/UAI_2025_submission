{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed977fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from sklearn import preprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd8ceb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ci_95(arr):\n",
    "    \"\"\"\n",
    "    Helper function to calculate 95% interval given an array\n",
    "    \"\"\"\n",
    "    return [np.percentile(arr, 2.5), np.percentile(arr, 97.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f898d2",
   "metadata": {},
   "source": [
    "# Performing Causal Analysis (Mortality after 28 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9c279c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving merged_data (look at tte_paper_experimentations_orig.ipynb on how to generate)\n",
    "full_data_df = pd.read_csv(\"full_data_df_no_index.csv\")\n",
    "\n",
    "### This is for non-feng annotated mimic data only\n",
    "pred_mimic_df = pd.read_csv(\"...\")\n",
    "\n",
    "### This is for n2c2 data only\n",
    "# pred_mimic_df = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88118f24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c04070",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mimic_df = pred_mimic_df.rename(columns={'SUBJECT_ID': 'subject_id'})\n",
    "pred_mimic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d4b34a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(full_data_df, pred_mimic_df[[\"subject_id\",\"REGEX_SMOKING_STATUS\"]], on=[\"subject_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c6ce87fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REGEX_SMOKING_STATUS\n",
       "3    2587\n",
       "1    1275\n",
       "2     686\n",
       "4     251\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"REGEX_SMOKING_STATUS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "41a900c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REGEX_SMOKING_STATUS\n",
       "3    2587\n",
       "1    1275\n",
       "2     686\n",
       "4     251\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Droppping 0 labels\n",
    "merged_df = merged_df.drop(merged_df[merged_df[\"REGEX_SMOKING_STATUS\"] == 0].index)\n",
    "merged_df[\"REGEX_SMOKING_STATUS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6f986148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting integers to weekdays\n",
    "def int_to_weekday(row):\n",
    "    r = int(row)\n",
    "    if r == 0:\n",
    "        return 'sunday'\n",
    "    elif r == 1:\n",
    "        return \"monday\"\n",
    "    elif r == 2:\n",
    "        return \"tuesday\"\n",
    "    elif r == 3:\n",
    "        return \"wednesday\"\n",
    "    elif r == 4:\n",
    "        return \"thursday\"\n",
    "    elif r== 5:\n",
    "        return \"friday\"\n",
    "    else:\n",
    "        return \"saturday\"\n",
    "\n",
    "merged_df[\"icu_adm_weekday\"] = merged_df[\"icu_adm_weekday\"].apply(int_to_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d22ae029",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"first_careunit\"] = merged_df[\"first_careunit\"].astype('category')\n",
    "merged_df[\"first_careunit\"] = merged_df[\"first_careunit\"].cat.reorder_categories([\"SICU\", \"MICU\"])\n",
    "\n",
    "merged_df[\"gender\"] = merged_df[\"gender\"].astype(\"category\")\n",
    "merged_df[\"gender\"] = merged_df[\"gender\"].cat.reorder_categories([\"M\", \"F\"])\n",
    "\n",
    "merged_df[\"icu_adm_weekday\"] = merged_df[\"icu_adm_weekday\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "66d3a82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.2526978 , -0.11653691, -0.0773134 , -0.05884749],\n",
       "       [-0.17243309,  1.27561676, -0.06752461, -0.03565906],\n",
       "       [-0.01687261, -0.04003172,  1.12912971, -0.07222539],\n",
       "       [-0.15851734, -0.09530782, -0.25136472,  1.50518988]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understanding Matrix of Error Adjustments\n",
    "# confusion = [\n",
    "#                 [9, 1, 1, 0],\n",
    "#                 [2, 9 ,0, 0],\n",
    "#                 [0, 0, 16, 0],\n",
    "#                 [0, 0, 0, 63]\n",
    "#             ] # rows represent the ground truth labels and cols represents the predicted labels\n",
    "\n",
    "# error_mat = [\n",
    "#                 [9/11, 1/11, 1/11, 0],\n",
    "#                 [2/11, 9/11, 0, 0],\n",
    "#                 [0, 0, 1, 0],\n",
    "#                 [0, 0, 0, 1]\n",
    "#             ] # rows represent U* and cols represent U\n",
    "\n",
    "\n",
    "confusion = [\n",
    "                [154, 15, 13, 7],\n",
    "                [13, 91, 7, 3],\n",
    "                [8, 12, 321, 16],\n",
    "                [3, 2, 5, 21]\n",
    "            ]\n",
    "\n",
    "error_mat = [\n",
    "                [154/189, 15/189, 13/189, 7/189],\n",
    "                [13/114, 91/114, 7/114, 3/114],\n",
    "                [8/357, 12/357, 321/357, 16/357],\n",
    "                [3/31, 2/31, 5/31, 21/31]\n",
    "            ]\n",
    "\n",
    "inverse = np.linalg.inv(error_mat)\n",
    "inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a8b14608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_models(dataframe):\n",
    "    '''\n",
    "    Given a pre-processed MIMIC + proxy prediction dataframe, train three logistic regression models \n",
    "    using smf.logit. \n",
    "    The formula strings will be hard-coded into the function. The assumptions for these models are:\n",
    "        1) Categorical smoking categories \n",
    "        2) Not all feature are binary, but at least the output (mort_28_day) and treatment (echo) \n",
    "           should be binary\n",
    "    '''\n",
    "    \n",
    "    # Calculating P(y | u*, a, c) --> y ~ u* + a + c for each u* label in [1,2,3,4] \n",
    "    fstring = 'mort_28_day ~ echo + age + weight + saps + sofa + elix_score + vent + \\\n",
    "            vaso + icu_adm_hour + icd_chf + icd_afib + icd_renal + icd_liver + icd_copd + \\\n",
    "            icd_cad + icd_stroke + icd_malignancy + vs_heart_rate_first + vs_map_first + vs_temp_first + \\\n",
    "            lab_hemoglobin_first + lab_platelet_first + lab_wbc_first + lab_chloride_first + \\\n",
    "            lab_sodium_first + lab_bun_first + lab_bicarbonate_first + lab_creatinine_first + \\\n",
    "            lab_potassium_first + vs_cvp_flag + lab_creatinine_kinase_flag + lab_bnp_flag + gender + \\\n",
    "            lab_troponin_flag + first_careunit + icu_adm_weekday + lab_ph_first + lab_pco2_first + \\\n",
    "            lab_po2_first + lab_lactate_first + sedative + C(REGEX_SMOKING_STATUS)'\n",
    "    eq1 = smf.logit(fstring, data=dataframe)\n",
    "    eq1_model = eq1.fit(disp=0)\n",
    "    \n",
    "    # Calculating P(u* | a, c)\n",
    "    f_string2 = \"REGEX_SMOKING_STATUS ~ echo + first_careunit + age + gender + weight + saps + sofa + elix_score + \\\n",
    "            vent + vaso + icu_adm_weekday + icu_adm_hour + icd_chf + icd_afib + icd_renal + icd_liver + \\\n",
    "            icd_copd + icd_cad + icd_stroke + icd_malignancy + vs_heart_rate_first + vs_map_first + \\\n",
    "            vs_temp_first + lab_hemoglobin_first + lab_platelet_first + lab_wbc_first + lab_ph_first + \\\n",
    "            lab_sodium_first + lab_bun_first + lab_bicarbonate_first + lab_pco2_first + lab_creatinine_first + \\\n",
    "            lab_chloride_first + lab_potassium_first + lab_po2_first + lab_lactate_first + sedative + \\\n",
    "            vs_cvp_flag + lab_creatinine_kinase_flag + lab_bnp_flag\"\n",
    "    eq2 = smf.mnlogit(f_string2, data=dataframe)\n",
    "    eq2_model = eq2.fit(disp=0)\n",
    "    \n",
    "    # Calculating P(a|c)\n",
    "    f_string3 = \"echo ~ first_careunit + age + gender + weight + saps + sofa + elix_score + vent + \\\n",
    "                vaso + icu_adm_weekday + icu_adm_hour + icd_chf + icd_afib + icd_renal + icd_liver + icd_copd + \\\n",
    "                icd_cad + icd_stroke + icd_malignancy + vs_heart_rate_first + vs_map_first + vs_temp_first + \\\n",
    "                lab_hemoglobin_first + lab_platelet_first + lab_wbc_first + lab_ph_first + lab_chloride_first + \\\n",
    "                lab_sodium_first + lab_bun_first + lab_bicarbonate_first + lab_pco2_first + lab_creatinine_first + \\\n",
    "                lab_potassium_first + lab_po2_first + lab_lactate_first + sedative + vs_cvp_flag + \\\n",
    "                lab_creatinine_kinase_flag + lab_bnp_flag + lab_troponin_flag\"\n",
    "    eq3 = smf.logit(f_string3, data=dataframe)\n",
    "    eq3_model = eq3.fit(disp=0)\n",
    "    \n",
    "    return eq1_model, eq2_model, eq3_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3db7275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_ratio(dataframe, model1, model2, model3):\n",
    "    '''\n",
    "    Given a pre-procesesed MIMIC + smoking proxy prediction dataframe as well as three trained models \n",
    "    from generate_models(), calculate the risk ratio as defined by: \n",
    "    causal_effect = summation(c,u){ p(c,u) * ( E[Y=1 | A=1,c,u*] / E[Y=1 | A=0,c,u*] ) }\n",
    "    The assumptions of this function are:\n",
    "        1) Smoking proxy predictions are categorical\n",
    "        2) Treatment variable values are binary --> either 1 for receiving treatment or 0 for not \n",
    "           receiving treatment\n",
    "        3) Order for model inputs matter:\n",
    "            a) model1 = P(y | u*, a, c) --> y ~ u* + a + c\n",
    "            b) model2 = P(u* | a, c)\n",
    "            c) model3 = P(a | c)\n",
    "        4) Default prediction is probability of getting 1 due to how statsmodels works\n",
    "    '''\n",
    "    \n",
    "    tmp_df = None\n",
    "    unique_smoking = [1,2,3,4]\n",
    "    unique_echo = [1,0]\n",
    "    exp_array = []\n",
    "    \n",
    "    confusion = [\n",
    "                    [154, 15, 13, 7],\n",
    "                    [13, 91, 7, 3],\n",
    "                    [8, 12, 321, 16],\n",
    "                    [3, 2, 5, 21]\n",
    "                ]\n",
    "\n",
    "    error_mat = [\n",
    "                    [154/189, 15/189, 13/189, 7/189],\n",
    "                    [13/114, 91/114, 7/114, 3/114],\n",
    "                    [8/357, 12/357, 321/357, 16/357],\n",
    "                    [3/31, 2/31, 5/31, 21/31]\n",
    "                ]\n",
    "\n",
    "\n",
    "#     confusion = [\n",
    "#                     [9, 1, 1, 0],\n",
    "#                     [2, 9, 0, 0],\n",
    "#                     [0, 0, 16, 0],\n",
    "#                     [0, 0, 0, 63]\n",
    "#                 ] # rows represent the ground truth labels and cols represents the predicted labels\n",
    "\n",
    "#     error_mat = [\n",
    "#                     [9/11, 1/11, 1/11, 0/11],\n",
    "#                     [2/11, 9/11, 0/11, 0],\n",
    "#                     [0/16, 0, 16/16, 0/16],\n",
    "#                     [0, 0, 0, 63/63]\n",
    "#                 ] # rows represent U* and cols represent U\n",
    "    inverse = np.linalg.pinv(error_mat)\n",
    "    # Getting P(A, c, y=1, u*) \n",
    "    prob_a1_c_y1_u = []\n",
    "    prob_a0_c_y1_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to be a cateogrical value in [1,2,3,4]\n",
    "        tmp_df[\"REGEX_SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model3.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y1_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y1_u.append(output)\n",
    "    \n",
    "    # Getting P(A, c, y=0, u*)\n",
    "    prob_a1_c_y0_u = []\n",
    "    prob_a0_c_y0_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to be a cateogrical value in [1,2,3,4]\n",
    "        tmp_df[\"REGEX_SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = 1 - model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model3.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y0_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y0_u.append(output)\n",
    "        \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=0)\n",
    "    num_0a = prob_a1_c_y1_u[0] * inverse[0][0] + prob_a1_c_y1_u[1] * inverse[1][0] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][0] + prob_a1_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0a = prob_a1_c_y0_u[0] * inverse[0][0] + prob_a1_c_y0_u[1] * inverse[1][0] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][0] + prob_a1_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0a = num_0a + tmp_0a\n",
    "    upper_0a = num_0a / denom_0a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=0)\n",
    "    num_0b = prob_a0_c_y1_u[0] * inverse[0][0] + prob_a0_c_y1_u[1] * inverse[1][0] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][0] + prob_a0_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0b = prob_a0_c_y0_u[0] * inverse[0][0] + prob_a0_c_y0_u[1] * inverse[1][0] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][0] + prob_a0_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0b = num_0b + tmp_0b\n",
    "    lower_0b = num_0b / denom_0b\n",
    "    \n",
    "    comp_0 = upper_0a / lower_0b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=1)\n",
    "    num_1a = prob_a1_c_y1_u[0] * inverse[0][1] + prob_a1_c_y1_u[1] * inverse[1][1] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][1] + prob_a1_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1a = prob_a1_c_y0_u[0] * inverse[0][1] + prob_a1_c_y0_u[1] * inverse[1][1] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][1] + prob_a1_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1a = num_1a + tmp_1a\n",
    "    upper_1a = num_1a / denom_1a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=1)\n",
    "    num_1b = prob_a0_c_y1_u[0] * inverse[0][1] + prob_a0_c_y1_u[1] * inverse[1][1] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][1] + prob_a0_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1b = prob_a0_c_y0_u[0] * inverse[0][1] + prob_a0_c_y0_u[1] * inverse[1][1] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][1] + prob_a0_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1b = num_1b + tmp_1b\n",
    "    lower_1b = num_1b / denom_1b\n",
    "    \n",
    "    comp_1 = upper_1a / lower_1b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=2)\n",
    "    num_2a = prob_a1_c_y1_u[0] * inverse[0][2] + prob_a1_c_y1_u[1] * inverse[1][2] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][2] + prob_a1_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2a = prob_a1_c_y0_u[0] * inverse[0][2] + prob_a1_c_y0_u[1] * inverse[1][2] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][2] + prob_a1_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2a = num_2a + tmp_2a\n",
    "    upper_2a = num_2a / denom_2a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=2)\n",
    "    num_2b = prob_a0_c_y1_u[0] * inverse[0][2] + prob_a0_c_y1_u[1] * inverse[1][2] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][2] + prob_a0_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2b = prob_a0_c_y0_u[0] * inverse[0][2] + prob_a0_c_y0_u[1] * inverse[1][2] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][2] + prob_a0_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2b = num_2b + tmp_2b\n",
    "    lower_2b = num_2b / denom_2b\n",
    "    \n",
    "    comp_2 = upper_2a / lower_2b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=3)\n",
    "    num_3a = prob_a1_c_y1_u[0] * inverse[0][3] + prob_a1_c_y1_u[1] * inverse[1][3] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][3] + prob_a1_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3a = prob_a1_c_y0_u[0] * inverse[0][3] + prob_a1_c_y0_u[1] * inverse[1][3] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][3] + prob_a1_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3a = num_3a + tmp_3a\n",
    "    upper_3a = num_3a / denom_3a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=3)\n",
    "    num_3b = prob_a0_c_y1_u[0] * inverse[0][3] + prob_a0_c_y1_u[1] * inverse[1][3] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][3] + prob_a0_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3b = prob_a0_c_y0_u[0] * inverse[0][3] + prob_a0_c_y0_u[1] * inverse[1][3] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][3] + prob_a0_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3b = num_3b + tmp_3b\n",
    "    lower_3b = num_3b / denom_3b\n",
    "    \n",
    "    comp_3 = upper_3a / lower_3b\n",
    "    \n",
    "    # Getting P(u | c) \n",
    "    prob_u0_c = num_0a + tmp_0a + num_0b + tmp_0b\n",
    "    prob_u1_c = num_1a + tmp_1a + num_1b + tmp_1b\n",
    "    prob_u2_c = num_2a + tmp_2a + num_2b + tmp_2b\n",
    "    prob_u3_c = num_3a + tmp_3a + num_3b + tmp_3b\n",
    "    \n",
    "    \n",
    "    # Calculating total RR\n",
    "    total_upper = np.mean(upper_0a * prob_u0_c + upper_1a * prob_u1_c + upper_2a * prob_u2_c + \\\n",
    "                          upper_3a * prob_u3_c)\n",
    "    total_lower = np.mean(lower_0b * prob_u0_c + lower_1b * prob_u1_c + lower_2b * prob_u2_c + \\\n",
    "                          lower_3b * prob_u3_c)\n",
    "    rr = total_upper / total_lower\n",
    "\n",
    "    sub_array = [np.mean(upper_0a * prob_u0_c) / np.mean(lower_0b * prob_u0_c),\n",
    "                 np.mean(upper_1a * prob_u1_c) / np.mean(lower_1b * prob_u1_c),\n",
    "                 np.mean(upper_2a * prob_u2_c) / np.mean(lower_2b * prob_u2_c),\n",
    "                 np.mean(upper_3a * prob_u3_c) / np.mean(lower_3b * prob_u3_c)]\n",
    "    return rr, sub_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ae5d0f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9259522391401211,\n",
       " [0.9212235349523306,\n",
       "  0.9176090376244971,\n",
       "  0.9290040758424991,\n",
       "  0.9977297128018503])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1, m2, m3 = generate_models(merged_df)\n",
    "risk_ratio(merged_df, m1, m2, m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b67017",
   "metadata": {},
   "source": [
    "# Bootstrapping Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d3ea33dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9359623354217923"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bootstrap_merged_data_rr(dataframe, model1, model2, model3):\n",
    "    '''\n",
    "    Given a pre-procesesed MIMIC + smoking proxy prediction dataframe as well as three trained models \n",
    "    from generate_models(), perform bootstrapping by shuffling the merged dataframe for\n",
    "    risk ratio calculations. Iterations set to 100.\n",
    "    The assumptions of this function are:\n",
    "        1) Smoking proxy predictions are categorical\n",
    "        2) Treatment variable values are binary --> either 1 for receiving treatment or 0 \n",
    "           for not receiving treatment\n",
    "        3) Order for model inputs matter:\n",
    "            a) model1 = P(y | u*, a, c) --> y ~ u* + a + c\n",
    "            b) model2 = P(u* | a, c)\n",
    "            c) model3 = P(a | c)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    iterations = 100\n",
    "    output = []\n",
    "    sub_matrix = np.zeros((iterations, 4))\n",
    "    for i in range(iterations):\n",
    "        bt_df = dataframe.sample(frac=1, replace=True, ignore_index=True)\n",
    "        rr, sub_array = risk_ratio(bt_df, model1, model2, model3)\n",
    "        output.append(rr)\n",
    "\n",
    "        for idx, c in enumerate(sub_array):\n",
    "            sub_matrix[i, idx] = c\n",
    "        sub_avg = sub_matrix.mean(axis=0)\n",
    "        \n",
    "    res_dict = {\"bs_rr\": sum(output) / len(output), \"bs_arr_rr\": output, \"sub_avg_rr\": sub_avg, \\\n",
    "                \"sub_arr_rr\": sub_matrix}\n",
    "    \n",
    "    return res_dict\n",
    "\n",
    "bt_merged_rr = bootstrap_merged_data_rr(merged_df, m1, m2, m3)\n",
    "bt_merged_rr[\"bs_rr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a62488c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9342721057014611, 0.9375354608633897]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing 95% interval for risk ratio while boostrapping merged dataframe \n",
    "compute_ci_95(bt_merged_rr[\"bs_arr_rr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "012a9b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93006744, 0.92734631, 0.92583146, 0.94602682],\n",
       "       [0.93340351, 0.93214428, 0.92953915, 0.95000295]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing 95% interval for risk ratio subgroups while bootstrapping merged dataframe\n",
    "np.apply_along_axis(compute_ci_95, axis=0, arr=bt_merged_rr[\"sub_arr_rr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18fa90",
   "metadata": {},
   "source": [
    "# Bootstrapping Error Rate Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1c87a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_ratio_bootstrap(dataframe, model1, model2, model3, error_mat):\n",
    "    '''\n",
    "    Given a pre-procesesed MIMIC + smoking proxy prediction dataframe, three trained models \n",
    "    from generate_models(), and an error rate matrix, perform bootstrapping on error-rate matrix for\n",
    "    risk-ratio calculations. Helper function for bootstrap()\n",
    "    The assumptions of this function are:\n",
    "        1) Smoking proxy predictions are categorical\n",
    "        2) Treatment variable values are binary --> either 1 for receiving treatment or 0 \n",
    "           for not receiving treatment\n",
    "        3) Order for model inputs matter:\n",
    "            a) model1 = P(y | u*, a, c) --> y ~ u* + a + c\n",
    "            b) model2 = P(u* | a, c)\n",
    "            c) model3 = P(a | c)\n",
    "    '''\n",
    "    \n",
    "    tmp_df = None\n",
    "    unique_smoking = [1,2,3,4]\n",
    "    unique_echo = [1,0]\n",
    "    exp_array = []\n",
    "    \n",
    "    # Inversing Error Rate Matrices\n",
    "    inverse = np.linalg.pinv(error_mat)\n",
    "    # print(inverse)\n",
    "    \n",
    "    # Getting P(A, c, y=1, u*) \n",
    "    prob_a1_c_y1_u = []\n",
    "    prob_a0_c_y1_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to be a cateogrical value in [1,2,3,4]\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model3.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y1_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y1_u.append(output)\n",
    "    \n",
    "    # Getting P(A, c, y=0, u*)\n",
    "    prob_a1_c_y0_u = []\n",
    "    prob_a0_c_y0_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to be a cateogrical value in [1,2,3,4]\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = 1 - model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model3.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y0_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y0_u.append(output)\n",
    "        \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=0)\n",
    "    num_0a = prob_a1_c_y1_u[0] * inverse[0][0] + prob_a1_c_y1_u[1] * inverse[1][0] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][0] + prob_a1_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0a = prob_a1_c_y0_u[0] * inverse[0][0] + prob_a1_c_y0_u[1] * inverse[1][0] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][0] + prob_a1_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0a = num_0a + tmp_0a\n",
    "    upper_0a = num_0a / denom_0a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=0)\n",
    "    num_0b = prob_a0_c_y1_u[0] * inverse[0][0] + prob_a0_c_y1_u[1] * inverse[1][0] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][0] + prob_a0_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0b = prob_a0_c_y0_u[0] * inverse[0][0] + prob_a0_c_y0_u[1] * inverse[1][0] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][0] + prob_a0_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0b = num_0b + tmp_0b\n",
    "    lower_0b = num_0b / denom_0b\n",
    "    \n",
    "    # comp_0 = upper_0a / lower_0b\n",
    "    \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=1)\n",
    "    num_1a = prob_a1_c_y1_u[0] * inverse[0][1] + prob_a1_c_y1_u[1] * inverse[1][1] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][1] + prob_a1_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1a = prob_a1_c_y0_u[0] * inverse[0][1] + prob_a1_c_y0_u[1] * inverse[1][1] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][1] + prob_a1_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1a = num_1a + tmp_1a\n",
    "    upper_1a = num_1a / denom_1a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=1)\n",
    "    num_1b = prob_a0_c_y1_u[0] * inverse[0][1] + prob_a0_c_y1_u[1] * inverse[1][1] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][1] + prob_a0_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1b = prob_a0_c_y0_u[0] * inverse[0][1] + prob_a0_c_y0_u[1] * inverse[1][1] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][1] + prob_a0_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1b = num_1b + tmp_1b\n",
    "    lower_1b = num_1b / denom_1b\n",
    "    \n",
    "    # comp_1 = upper_1a / lower_1b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=2)\n",
    "    num_2a = prob_a1_c_y1_u[0] * inverse[0][2] + prob_a1_c_y1_u[1] * inverse[1][2] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][2] + prob_a1_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2a = prob_a1_c_y0_u[0] * inverse[0][2] + prob_a1_c_y0_u[1] * inverse[1][2] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][2] + prob_a1_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2a = num_2a + tmp_2a\n",
    "    upper_2a = num_2a / denom_2a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=2)\n",
    "    num_2b = prob_a0_c_y1_u[0] * inverse[0][2] + prob_a0_c_y1_u[1] * inverse[1][2] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][2] + prob_a0_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2b = prob_a0_c_y0_u[0] * inverse[0][2] + prob_a0_c_y0_u[1] * inverse[1][2] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][2] + prob_a0_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2b = num_2b + tmp_2b\n",
    "    lower_2b = num_2b / denom_2b\n",
    "    \n",
    "    # comp_2 = upper_2a / lower_2b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=3)\n",
    "    num_3a = prob_a1_c_y1_u[0] * inverse[0][3] + prob_a1_c_y1_u[1] * inverse[1][3] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][3] + prob_a1_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3a = prob_a1_c_y0_u[0] * inverse[0][3] + prob_a1_c_y0_u[1] * inverse[1][3] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][3] + prob_a1_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3a = num_3a + tmp_3a\n",
    "    upper_3a = num_3a / denom_3a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=3)\n",
    "    num_3b = prob_a0_c_y1_u[0] * inverse[0][3] + prob_a0_c_y1_u[1] * inverse[1][3] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][3] + prob_a0_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3b = prob_a0_c_y0_u[0] * inverse[0][3] + prob_a0_c_y0_u[1] * inverse[1][3] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][3] + prob_a0_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3b = num_3b + tmp_3b\n",
    "    lower_3b = num_3b / denom_3b\n",
    "    \n",
    "    # comp_3 = upper_3a / lower_3b\n",
    "    \n",
    "    # Getting P(u | c) \n",
    "    prob_u0_c = num_0a + tmp_0a + num_0b + tmp_0b\n",
    "    prob_u1_c = num_1a + tmp_1a + num_1b + tmp_1b\n",
    "    prob_u2_c = num_2a + tmp_2a + num_2b + tmp_2b\n",
    "    prob_u3_c = num_3a + tmp_3a + num_3b + tmp_3b\n",
    "    \n",
    "    total_upper = np.mean(upper_0a * prob_u0_c + upper_1a * prob_u1_c + upper_2a * prob_u2_c + \\\n",
    "                          upper_3a * prob_u3_c)\n",
    "    total_lower = np.mean(lower_0b * prob_u0_c + lower_1b * prob_u1_c + lower_2b * prob_u2_c + \\\n",
    "                          lower_3b * prob_u3_c)\n",
    "    \n",
    "    rr = total_upper / total_lower\n",
    "    sub_array = [np.mean(upper_0a * prob_u0_c) / np.mean(lower_0b * prob_u0_c),\n",
    "                 np.mean(upper_1a * prob_u1_c) / np.mean(lower_1b * prob_u1_c),\n",
    "                 np.mean(upper_2a * prob_u2_c) / np.mean(lower_2b * prob_u2_c),\n",
    "                 np.mean(upper_3a * prob_u3_c) / np.mean(lower_3b * prob_u3_c)]\n",
    "\n",
    "    return rr, sub_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c53e94eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(dataframe, model1, model2, model3):\n",
    "    '''\n",
    "    Given a dataframe and 3 models generated from generate_models(), bootstrap the testing set \n",
    "    for n2c2 2006 smoking dataset to get different error rate matrices to test robustness of the \n",
    "    risk ratio casual effect. Iterations set to 100\n",
    "    Utilize predict_bootstrap_2006.py to generate pickle files that store the confusion matrices.\n",
    "    '''\n",
    "    \n",
    "    # Iterating through the bootstrapped confusion matrices \n",
    "    # \"iterations\" var depends on how many bootstrapped confusion matrics were generated\n",
    "    iterations = 100\n",
    "    rr_arr = []\n",
    "    sub_matrix = np.zeros((iterations, 4))\n",
    "    cnt = 0\n",
    "    for x in range(0, iterations):\n",
    "        # Access each pickle file containing the confusion matrix\n",
    "        f = open(\"...\"+ str(x) + \".pkl\", \"rb\")\n",
    "        con_matrix = pickle.load(f)\n",
    "        if con_matrix.shape[0] > 4:\n",
    "            con_matrix = con_matrix[1:, 1:]\n",
    "        res = con_matrix/con_matrix.sum(axis=1)[:,None]\n",
    "        \n",
    "        rr, sub_array = risk_ratio_bootstrap(dataframe, model1, model2, model3, res)\n",
    "        rr_arr.append(rr)\n",
    "        flag = False\n",
    "        sub_avg = sub_matrix.mean(axis=0)\n",
    "\n",
    "    res_dict = {\"bt_rr\": sum(rr_arr) / len(rr_arr), \"bt_arr_rr\": rr_arr, \"sub_avg_rr\": sub_avg, \\\n",
    "                \"sub_arr_rr\": sub_matrix}\n",
    "    \n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad238fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m1, m2, m3 = generate_models(merged_df)\n",
    "bt_rr = bootstrap(merged_df, m1, m2, m3)\n",
    "bt_rr[\"bt_rr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "71ae40a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9137452846420341, 0.9489078503562732]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing 95% interval for risk ratio while boostrapping error rate matrix\n",
    "compute_ci_95(bt_rr[\"bt_arr_rr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "430a9754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91377539, 0.74762713, 0.9096855 , 0.94786187],\n",
       "       [0.95269814, 1.03432301, 0.93659898, 0.94786187]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing 95% interval for risk ratio subgroups while\n",
    "# bootstrapping error rate matrix\n",
    "np.apply_along_axis(compute_ci_95, axis=0, arr=bt_rr[\"sub_arr_rr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1513a0d0",
   "metadata": {},
   "source": [
    "# Combined Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2b876667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_bootstrap_rr(dataframe, model1, model2, model3):\n",
    "    '''\n",
    "    Given a dataframe and 3 models generated from generate_models(), do combined bootstrapping to see\n",
    "    robustness of risk ratio calculations. Iterations set to 100\n",
    "    Utilize predict_bootstrap_2006.py to generate pickle files that store the confusion matrices.\n",
    "    '''\n",
    "    \n",
    "    # Iterate through 10 of bootstrapped error matrices\n",
    "    iterations_em = 10\n",
    "    rr_arr = []\n",
    "    for iem in range(iterations_em):\n",
    "        f = open(\"...\"+ str(iem) + \".pkl\", \"rb\")\n",
    "        con_matrix = pickle.load(f)\n",
    "        if con_matrix.shape[0] > 4:\n",
    "            con_matrix = con_matrix[1:, 1:]\n",
    "        res = con_matrix/con_matrix.sum(axis=1)[:,None]\n",
    "        \n",
    "        # Iterate through 10 bootstrapped (shuffled) dataframes\n",
    "        iterations_df = 10\n",
    "        for idf in range(iterations_em):\n",
    "            bt_df = dataframe.sample(frac=1, replace=True, ignore_index=True)\n",
    "            rr, sub_array = risk_ratio_bootstrap(bt_df, model1, model2, model3, res)\n",
    "            rr_arr.append(rr)\n",
    "    print(\"Number of calcs:\", len(rr_arr)) # == 100 based on default settings\n",
    "    print(\"Mean combined bootstrap rr:\", sum(rr_arr) / len(rr_arr))\n",
    "    return [sum(rr_arr) / len(rr_arr), rr_arr]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5c8f1fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of calcs: 100\n",
      "Mean combined bootstrap rr: 0.9347642967618173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9031153984162691, 0.9542938326040282]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Compute 95% invertal for RR while doing combined bootstrapping\n",
    "combined_rr = combined_bootstrap_rr(merged_df, m1, m2, m3)\n",
    "compute_ci_95(combined_rr[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6cbf51",
   "metadata": {},
   "source": [
    "# Implementing OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8025cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odds_ratio(dataframe, model1, model2, model3):\n",
    "    '''\n",
    "    Given a pre-procesesed MIMIC + smoking proxy prediction dataframe as well as three trained models \n",
    "    from generate_models(), calculate the odds ratio as defined by: \n",
    "    causal_effect = (P(Y^{a=1}=1) * P(Y^{a=0}=0)) / (P(Y^{a=1}=0) * P(Y^{a=0}=1))\n",
    "    The assumptions of this function are:\n",
    "        1) Smoking proxy predictions are categorical\n",
    "        2) Treatment variable values are binary --> either 1 for receiving treatment or 0 \n",
    "           for not receiving treatment\n",
    "        3) Order for model inputs matter:\n",
    "            a) model1 = P(y | u*, a, c) --> y ~ u* + a + c\n",
    "            b) model2 = P(u* | a, c)\n",
    "            c) model3 = P(a | c)\n",
    "        4) Default prediction is probability of getting 1 due to how statsmodels works\n",
    "    '''\n",
    "    \n",
    "    tmp_df = None\n",
    "    unique_smoking = [1,2,3,4]\n",
    "    unique_echo = [1,0]\n",
    "    exp_array = []\n",
    "    \n",
    "    # Creating Matrix of Error Adjustments\n",
    "#     confusion = [\n",
    "#                     [9, 1, 1, 0],\n",
    "#                     [2, 9, 0, 0],\n",
    "#                     [0, 0, 16, 0],\n",
    "#                     [0, 0, 0, 63]\n",
    "#                 ] # rows represent the ground truth labels and cols represents the predicted labels\n",
    "\n",
    "#     error_mat = [\n",
    "#                     [9/11, 1/11, 1/11, 0/11],\n",
    "#                     [2/11, 9/11, 0/11, 0],\n",
    "#                     [0/16, 0, 16/16, 0/16],\n",
    "#                     [0, 0, 0, 63/63]\n",
    "#                 ] # rows represent U* and cols represent U\n",
    "    confusion = [\n",
    "                [167, 5, 17, 0],\n",
    "                [20, 70, 22, 1],\n",
    "                [13, 10, 330, 3],\n",
    "                [8, 0, 12, 11]\n",
    "            ] # rows represent the ground truth labels and cols represents the predicted labels\n",
    "\n",
    "    error_mat = [\n",
    "                    [167/189, 5/189, 17/189, 0/189],\n",
    "                    [20/113, 70/113, 22/113, 1/113],\n",
    "                    [13/356, 10/356, 330/356, 3/356],\n",
    "                    [8/31, 0, 12/31, 11/31]\n",
    "                ] # rows represent U* and cols represent U\n",
    "    inverse = np.linalg.inv(error_mat)\n",
    "    \n",
    "    # Getting P(A, c, y=1, u*) \n",
    "    prob_a1_c_y1_u = []\n",
    "    prob_a0_c_y1_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to be a cateogrical value in [1,2,3,4]\n",
    "        tmp_df[\"REGEX_SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model3.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y1_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y1_u.append(output)\n",
    "    \n",
    "    # Getting P(A, c, y=0, u*)\n",
    "    prob_a1_c_y0_u = []\n",
    "    prob_a0_c_y0_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to either be 1 or 0\n",
    "        tmp_df[\"REGEX_SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = 1 - model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model3.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y0_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y0_u.append(output)\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=0)\n",
    "    num_0a = prob_a1_c_y1_u[0] * inverse[0][0] + prob_a1_c_y1_u[1] * inverse[1][0] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][0] + prob_a1_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0a = prob_a1_c_y0_u[0] * inverse[0][0] + prob_a1_c_y0_u[1] * inverse[1][0] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][0] + prob_a1_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0a = num_0a + tmp_0a\n",
    "    upper_0a = num_0a / denom_0a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=0)\n",
    "    num_0b = prob_a0_c_y1_u[0] * inverse[0][0] + prob_a0_c_y1_u[1] * inverse[1][0] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][0] + prob_a0_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0b = prob_a0_c_y0_u[0] * inverse[0][0] + prob_a0_c_y0_u[1] * inverse[1][0] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][0] + prob_a0_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0b = num_0b + tmp_0b\n",
    "    lower_0b = num_0b / denom_0b\n",
    "    \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=1)\n",
    "    num_1a = prob_a1_c_y1_u[0] * inverse[0][1] + prob_a1_c_y1_u[1] * inverse[1][1] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][1] + prob_a1_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1a = prob_a1_c_y0_u[0] * inverse[0][1] + prob_a1_c_y0_u[1] * inverse[1][1] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][1] + prob_a1_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1a = num_1a + tmp_1a\n",
    "    upper_1a = num_1a / denom_1a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=1)\n",
    "    num_1b = prob_a0_c_y1_u[0] * inverse[0][1] + prob_a0_c_y1_u[1] * inverse[1][1] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][1] + prob_a0_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1b = prob_a0_c_y0_u[0] * inverse[0][1] + prob_a0_c_y0_u[1] * inverse[1][1] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][1] + prob_a0_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1b = num_1b + tmp_1b\n",
    "    lower_1b = num_1b / denom_1b\n",
    "    \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=2)\n",
    "    num_2a = prob_a1_c_y1_u[0] * inverse[0][2] + prob_a1_c_y1_u[1] * inverse[1][2] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][2] + prob_a1_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2a = prob_a1_c_y0_u[0] * inverse[0][2] + prob_a1_c_y0_u[1] * inverse[1][2] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][2] + prob_a1_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2a = num_2a + tmp_2a\n",
    "    upper_2a = num_2a / denom_2a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=2)\n",
    "    num_2b = prob_a0_c_y1_u[0] * inverse[0][2] + prob_a0_c_y1_u[1] * inverse[1][2] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][2] + prob_a0_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2b = prob_a0_c_y0_u[0] * inverse[0][2] + prob_a0_c_y0_u[1] * inverse[1][2] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][2] + prob_a0_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2b = num_2b + tmp_2b\n",
    "    lower_2b = num_2b / denom_2b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=3)\n",
    "    num_3a = prob_a1_c_y1_u[0] * inverse[0][3] + prob_a1_c_y1_u[1] * inverse[1][3] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][3] + prob_a1_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3a = prob_a1_c_y0_u[0] * inverse[0][3] + prob_a1_c_y0_u[1] * inverse[1][3] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][3] + prob_a1_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3a = num_3a + tmp_3a\n",
    "    upper_3a = num_3a / denom_3a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=3)\n",
    "    num_3b = prob_a0_c_y1_u[0] * inverse[0][3] + prob_a0_c_y1_u[1] * inverse[1][3] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][3] + prob_a0_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3b = prob_a0_c_y0_u[0] * inverse[0][3] + prob_a0_c_y0_u[1] * inverse[1][3] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][3] + prob_a0_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3b = num_3b + tmp_3b\n",
    "    lower_3b = num_3b / denom_3b\n",
    "    \n",
    "    # Getting P(u | c) \n",
    "    prob_u0_c = num_0a + tmp_0a + num_0b + tmp_0b\n",
    "    prob_u1_c = num_1a + tmp_1a + num_1b + tmp_1b\n",
    "    prob_u2_c = num_2a + tmp_2a + num_2b + tmp_2b\n",
    "    prob_u3_c = num_3a + tmp_3a + num_3b + tmp_3b\n",
    "    \n",
    "     \n",
    "    numerator_a = np.sum(upper_0a * prob_u0_c + upper_1a * prob_u1_c + upper_2a * prob_u2_c + \\\n",
    "                         upper_3a * prob_u3_c)\n",
    "    numerator_b = np.sum((1 - lower_0b) * prob_u0_c + (1 - lower_1b) * prob_u1_c + \\\n",
    "                         (1 - lower_2b) * prob_u2_c + (1 - lower_3b) * prob_u3_c)\n",
    "    \n",
    "    \n",
    "    denominator_a = np.sum((1 - upper_0a) * prob_u0_c + (1 - upper_1a) * prob_u1_c + \\\n",
    "                           (1 - upper_2a) * prob_u2_c + (1 - upper_3a) * prob_u3_c)\n",
    "    denominator_b = np.sum(lower_0b * prob_u0_c + lower_1b * prob_u1_c + lower_2b * prob_u2_c + \\\n",
    "                           lower_3b * prob_u3_c)\n",
    "    \n",
    "    numerator = numerator_a * numerator_b\n",
    "    denominator = denominator_a * denominator_b\n",
    "    \n",
    "    return numerator / denominator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ca656b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8958690435797695"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1, m2, m3 = generate_models(merged_df)\n",
    "odds_ratio(merged_df, m1, m2, m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56844e59",
   "metadata": {},
   "source": [
    "# Bootstrapping for OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "05c1a7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90649159185656"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bootstrap_merged_data_or(dataframe, m1, m2, m3):\n",
    "    '''\n",
    "    Given a pre-procesesed MIMIC + smoking proxy prediction dataframe as well as three trained models \n",
    "    from generate_models(), perform bootstrapping by shuffling the merged dataframe for\n",
    "    odds ratio calculations. Iterations set to 100.\n",
    "    The assumptions of this function are:\n",
    "        1) Smoking proxy predictions are categorical\n",
    "        2) Treatment variable values are binary --> either 1 for receiving treatment or 0 \n",
    "           for not receiving treatment\n",
    "        3) Order for model inputs matter:\n",
    "            a) model1 = P(y | u*, a, c) --> y ~ u* + a + c\n",
    "            b) model2 = P(u* | a, c)\n",
    "            c) model3 = P(a | c)\n",
    "    '''\n",
    "    iterations = 100\n",
    "    output = []\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        bt_data = dataframe.sample(frac=1, replace=True, ignore_index=True)\n",
    "        or_val = odds_ratio(bt_data, m1, m2, m3)\n",
    "        output.append(or_val)\n",
    "\n",
    "    return [sum(output) / len(output), output]\n",
    "\n",
    "bt_merged_or = bootstrap_merged_data_or(merged_df, m1, m2, m3)\n",
    "bt_merged_or[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6e87266c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9043440681431902, 0.9082677881911791]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute 95% CI for OR while bootstrapping merged dataframe\n",
    "compute_ci_95(bt_merged_or[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c64d8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odds_ratio_bootstrap(dataframe, model1, model2, model3, error_mat):\n",
    "    '''\n",
    "    Given a pre-procesesed MIMIC + smoking proxy prediction dataframe, three trained models \n",
    "    from generate_models(), and an error rate matrix, perform bootstrapping on error-rate matrix for\n",
    "    odds ratio calculations. Helper function for bootstrap_or()\n",
    "    The assumptions of this function are:\n",
    "        1) Smoking proxy predictions are categorical\n",
    "        2) Treatment variable values are binary --> either 1 for receiving treatment or 0 \n",
    "           for not receiving treatment\n",
    "        3) Order for model inputs matter:\n",
    "            a) model1 = P(y | u*, a, c) --> y ~ u* + a + c\n",
    "            b) model2 = P(u* | a, c)\n",
    "            c) model3 = P(a | c)\n",
    "    '''\n",
    "    tmp_df = None\n",
    "    unique_smoking = [1,2,3,4]\n",
    "    unique_echo = [1,0]\n",
    "    exp_array = []\n",
    "    \n",
    "    inverse = np.linalg.pinv(error_mat)\n",
    "    \n",
    "    # Getting P(A, c, y=1, u*) \n",
    "    prob_a1_c_y1_u = []\n",
    "    prob_a0_c_y1_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to be a cateogrical value in [1,2,3,4]\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model3.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y1_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y1_u.append(output)\n",
    "    \n",
    "    # Getting P(A, c, y=0, u*)\n",
    "    prob_a1_c_y0_u = []\n",
    "    prob_a0_c_y0_u = []\n",
    "    for s in unique_smoking:\n",
    "        tmp_df = copy.deepcopy(dataframe)\n",
    "    \n",
    "        # Presetting the smoking status in the dataframe to either be 1 or 0\n",
    "        tmp_df[\"SMOKING_STATUS\"] = [s] * tmp_df.shape[0]\n",
    "        \n",
    "        for e in unique_echo:\n",
    "            tmp_tmp_df = copy.deepcopy(tmp_df)\n",
    "            tmp_tmp_df[\"echo\"] = [e] * tmp_df.shape[0]\n",
    "            \n",
    "            prob_1 = 1 - model1.predict(tmp_tmp_df)\n",
    "            prob_2 = model2.predict(tmp_tmp_df)[:][s-1]\n",
    "            prob_3 = model3.predict(tmp_tmp_df)\n",
    "            \n",
    "            \n",
    "            if e == 0:\n",
    "                output = prob_1 * prob_2 * (1 - prob_3)\n",
    "                prob_a0_c_y0_u.append(output)\n",
    "            else:\n",
    "                output = prob_1 * prob_2 * prob_3\n",
    "                prob_a1_c_y0_u.append(output)\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=0)\n",
    "    num_0a = prob_a1_c_y1_u[0] * inverse[0][0] + prob_a1_c_y1_u[1] * inverse[1][0] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][0] + prob_a1_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0a = prob_a1_c_y0_u[0] * inverse[0][0] + prob_a1_c_y0_u[1] * inverse[1][0] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][0] + prob_a1_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0a = num_0a + tmp_0a\n",
    "    upper_0a = num_0a / denom_0a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=0)\n",
    "    num_0b = prob_a0_c_y1_u[0] * inverse[0][0] + prob_a0_c_y1_u[1] * inverse[1][0] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][0] + prob_a0_c_y1_u[3] * inverse[3][0]\n",
    "    tmp_0b = prob_a0_c_y0_u[0] * inverse[0][0] + prob_a0_c_y0_u[1] * inverse[1][0] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][0] + prob_a0_c_y0_u[3] * inverse[3][0]\n",
    "    denom_0b = num_0b + tmp_0b\n",
    "    lower_0b = num_0b / denom_0b\n",
    "    \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=1)\n",
    "    num_1a = prob_a1_c_y1_u[0] * inverse[0][1] + prob_a1_c_y1_u[1] * inverse[1][1] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][1] + prob_a1_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1a = prob_a1_c_y0_u[0] * inverse[0][1] + prob_a1_c_y0_u[1] * inverse[1][1] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][1] + prob_a1_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1a = num_1a + tmp_1a\n",
    "    upper_1a = num_1a / denom_1a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=1)\n",
    "    num_1b = prob_a0_c_y1_u[0] * inverse[0][1] + prob_a0_c_y1_u[1] * inverse[1][1] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][1] + prob_a0_c_y1_u[3] * inverse[3][1]\n",
    "    tmp_1b = prob_a0_c_y0_u[0] * inverse[0][1] + prob_a0_c_y0_u[1] * inverse[1][1] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][1] + prob_a0_c_y0_u[3] * inverse[3][1]\n",
    "    denom_1b = num_1b + tmp_1b\n",
    "    lower_1b = num_1b / denom_1b\n",
    "    \n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=2)\n",
    "    num_2a = prob_a1_c_y1_u[0] * inverse[0][2] + prob_a1_c_y1_u[1] * inverse[1][2] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][2] + prob_a1_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2a = prob_a1_c_y0_u[0] * inverse[0][2] + prob_a1_c_y0_u[1] * inverse[1][2] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][2] + prob_a1_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2a = num_2a + tmp_2a\n",
    "    upper_2a = num_2a / denom_2a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=2)\n",
    "    num_2b = prob_a0_c_y1_u[0] * inverse[0][2] + prob_a0_c_y1_u[1] * inverse[1][2] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][2] + prob_a0_c_y1_u[3] * inverse[3][2]\n",
    "    tmp_2b = prob_a0_c_y0_u[0] * inverse[0][2] + prob_a0_c_y0_u[1] * inverse[1][2] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][2] + prob_a0_c_y0_u[3] * inverse[3][2]\n",
    "    denom_2b = num_2b + tmp_2b\n",
    "    lower_2b = num_2b / denom_2b\n",
    "    \n",
    "    # Getting P(Y=1 | A=1, C, U=3)\n",
    "    num_3a = prob_a1_c_y1_u[0] * inverse[0][3] + prob_a1_c_y1_u[1] * inverse[1][3] + prob_a1_c_y1_u[2] * \\\n",
    "             inverse[2][3] + prob_a1_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3a = prob_a1_c_y0_u[0] * inverse[0][3] + prob_a1_c_y0_u[1] * inverse[1][3] + prob_a1_c_y0_u[2] * \\\n",
    "             inverse[2][3] + prob_a1_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3a = num_3a + tmp_3a\n",
    "    upper_3a = num_3a / denom_3a\n",
    "    \n",
    "    # Getting P(Y=1 | A=0, C, U=3)\n",
    "    num_3b = prob_a0_c_y1_u[0] * inverse[0][3] + prob_a0_c_y1_u[1] * inverse[1][3] + prob_a0_c_y1_u[2] * \\\n",
    "             inverse[2][3] + prob_a0_c_y1_u[3] * inverse[3][3]\n",
    "    tmp_3b = prob_a0_c_y0_u[0] * inverse[0][3] + prob_a0_c_y0_u[1] * inverse[1][3] + prob_a0_c_y0_u[2] * \\\n",
    "             inverse[2][3] + prob_a0_c_y0_u[3] * inverse[3][3]\n",
    "    denom_3b = num_3b + tmp_3b\n",
    "    lower_3b = num_3b / denom_3b\n",
    "    \n",
    "    # Getting P(u | c) \n",
    "    prob_u0_c = num_0a + tmp_0a + num_0b + tmp_0b\n",
    "    prob_u1_c = num_1a + tmp_1a + num_1b + tmp_1b\n",
    "    prob_u2_c = num_2a + tmp_2a + num_2b + tmp_2b\n",
    "    prob_u3_c = num_3a + tmp_3a + num_3b + tmp_3b\n",
    "    \n",
    "    #     numerator_a = np.sum(upper_0a * prob_u0_c) + np.sum(upper_1a * prob_u1_c) + \\\n",
    "    #                   np.sum(upper_2a * prob_u2_c) + np.sum(upper_3a * prob_u3_c)\n",
    "    #     numerator_b = np.sum((1 - lower_0b) * prob_u0_c) + np.sum((1 - lower_1b) * prob_u1_c) + \\\n",
    "    #                   np.sum((1 - lower_2b) * prob_u2_c) + np.sum((1 - lower_3b) * prob_u3_c)\n",
    "     \n",
    "    numerator_a = np.sum(upper_0a * prob_u0_c + upper_1a * prob_u1_c + upper_2a * prob_u2_c + \\\n",
    "                         upper_3a * prob_u3_c)\n",
    "    numerator_b = np.sum((1 - lower_0b) * prob_u0_c + (1 - lower_1b) * prob_u1_c + \\\n",
    "                         (1 - lower_2b) * prob_u2_c + (1 - lower_3b) * prob_u3_c)\n",
    "    \n",
    "    \n",
    "    #     denominator_a = np.sum((1 - upper_0a) * prob_u0_c) + np.sum((1 - upper_1a) * prob_u1_c) + \\\n",
    "    #                     np.sum((1 - upper_2a) * prob_u2_c) + np.sum((1 - upper_3a) * prob_u3_c)\n",
    "    #     denominator_b = np.sum(lower_0b * prob_u0_c) + np.sum(lower_1b * prob_u1_c) + \\\n",
    "    #                     np.sum(lower_2b * prob_u2_c) + np.sum(lower_3b * prob_u3_c)\n",
    "    \n",
    "    denominator_a = np.sum((1 - upper_0a) * prob_u0_c + (1 - upper_1a) * prob_u1_c + \\\n",
    "                           (1 - upper_2a) * prob_u2_c + (1 - upper_3a) * prob_u3_c)\n",
    "    denominator_b = np.sum(lower_0b * prob_u0_c + lower_1b * prob_u1_c + lower_2b * prob_u2_c + \\\n",
    "                           lower_3b * prob_u3_c)\n",
    "    \n",
    "    numerator = numerator_a * numerator_b\n",
    "    denominator = denominator_a * denominator_b\n",
    "    \n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6785d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_or(dataframe, model1, model2, model3):\n",
    "    '''\n",
    "    Given a dataframe and 3 models generated from generate_models(), bootstrap the testing set \n",
    "    for n2c2 2006 smoking dataset to get different error rate matrices to test robustness of \n",
    "    the odds ratio casual effect.\n",
    "    Utilize predict_bootstrap_2006.py to generate pickle files that store the confusion matrices.\n",
    "    '''\n",
    "    \n",
    "    # Iterating through the bootstrapped confusion matrices \n",
    "    # \"iterations\" var depends on how many bootstrapped confusion matrics were generated\n",
    "    # Default in predict_bootstrap_2006.py is 10\n",
    "    iterations = 100\n",
    "    o_r_arr = []\n",
    "    for x in range(iterations):\n",
    "        f = open(\"...\"+ str(x) + \".pkl\", \"rb\")\n",
    "        con_matrix = pickle.load(f)\n",
    "        print(x, con_matrix.shape)\n",
    "        if con_matrix.shape[0] > 4:\n",
    "            con_matrix = con_matrix[1:, 1:]\n",
    "        res = con_matrix/con_matrix.sum(axis=1)[:,None]\n",
    "        o_r = odds_ratio_bootstrap(dataframe, model1, model2, model3, res)\n",
    "        o_r_arr.append(o_r)\n",
    "    \n",
    "    return [sum(o_r_arr) / len(o_r_arr), o_r_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde73563",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m1, m2, m3 = generate_models(merged_df)\n",
    "bt_or = bootstrap_or(merged_df, m1, m2, m3)\n",
    "bt_or[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "38d336ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8749387480301432, 0.9250413083620885]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing 95% interval for odds ratio while boostrapping error rate matrix\n",
    "compute_ci_95(bt_or[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4f4a8a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_bootstrap_or(dataframe, model1, model2, model3):\n",
    "    '''\n",
    "    Given a dataframe and 3 models generated from generate_models(), do combined bootstrapping to see\n",
    "    robustness of odds ratio calculations. Iterations set to 100\n",
    "    Utilize predict_bootstrap_2006.py to generate pickle files that store the confusion matrices.\n",
    "    '''\n",
    "    \n",
    "    # Iterate through 10 of bootstrapped error matrices\n",
    "    iterations_em = 10\n",
    "    or_arr = []\n",
    "    for iem in range(iterations_em):\n",
    "        f = open(\"...\"+ str(iem) + \".pkl\", \"rb\")\n",
    "        con_matrix = pickle.load(f)\n",
    "        if con_matrix.shape[0] > 4:\n",
    "            con_matrix = con_matrix[1:, 1:]\n",
    "        res = con_matrix/con_matrix.sum(axis=1)[:,None]\n",
    "        \n",
    "        # Iterate through 10 bootstrapped (shuffled) dataframes\n",
    "        iterations_df = 10\n",
    "        for idf in range(iterations_df):\n",
    "            bt_df = dataframe.sample(frac=1, replace=True, ignore_index=True)\n",
    "            or_v = odds_ratio_bootstrap(bt_df, model1, model2, model3, res)\n",
    "            or_arr.append(or_v)\n",
    "    print(len(or_arr)) # == 100 based on default settings\n",
    "    print(\"Mean combined bootstrap or_v:\", sum(or_arr) / len(or_arr))\n",
    "    return [sum(or_arr) / len(or_arr), or_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b6a387f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Mean combined bootstrap or_v: 0.9044373192163426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8664058771592411, 0.931047478473314]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Compute 95% invertal for OR while doing combined bootstrapping\n",
    "combined_or = combined_bootstrap_or(merged_df, m1, m2, m3)\n",
    "compute_ci_95(combined_or[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79dec32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
